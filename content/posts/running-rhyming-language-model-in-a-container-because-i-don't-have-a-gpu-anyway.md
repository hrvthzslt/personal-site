---
title: "Running a Rhyming Language Model in a Container Because I Don't Have a GPU Anyway"
date: 2025-03-03T07:37:00Z
draft: true
---

Some of the conversation around AI I found interesting but more of it I found tiring. What I never find boring is running technologies on your own machine.

So I was delighted when I tried **Ollama** and created my own Shakespeare.

<!--more-->

## Ollama

After two months of caring about a baby pet not a pet project (well my projects are more like pet of a pet project in scope, but that is beside the point), I wanted to try something new. I already knew about **Ollama**, but the first shock was how easy is it to install and run. With appropiate hardware it is possible to set up your own "Home AI" with multiple models, without any hassle.

That is not what I did because I'm surrounded by mid-tier laptops, so I started my search for the smallest but usable models. Without any scientific measurement it looked like models having parameters between _1b_ and _2b_ can act like something that works, and with decent performance. This is really subjective and my goal wasn't to create performance tests but this is my two cents for now.

